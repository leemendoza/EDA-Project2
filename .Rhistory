myCorpus = tm_map(myCorpus, tolower)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
## remove numbers
## TODO: DECIDE IF YOU NEED TO REMOVE NUMBERS, OR IF YOU NEED THEM
## SKIP THIS STEP!!!
myCorpus <- tm_map(myCorpus, removeNumbers)
## inspecting the corpus
inspect(myCorpus[4:10])
## remove URLs
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
## replace '/', used sometimes to separate alternatives w a space
## replace '@' in email addresses
## replace '\\' from URLS
for (j in seq(myCorpus))
{
myCorpus[[j]] <- gsub("/", " ", myCorpus[[j]])
myCorpus[[j]] <- gsub("@", " ", myCorpus[[j]])
myCorpus[[j]] <- gsub("\\|", " ", myCorpus[[j]])
}
## add two extra stop words: "available" and "via"
myStopwords <- c(stopwords('english'), "available", "via")
## remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, c("r", "big"))
## remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
## strip whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
## task-specific transformations (pw to pathway, etc)
# for (j in seq(myCorpus))
# {
#     myCorpus[[j]] <- gsub("harbin institute technology", "HIT", myCorpus[[j]])
#     myCorpus[[j]] <- gsub("shenzhen institutes advanced technology", "SIAT", myCorpus[[j]])
#     myCorpus[[j]] <- gsub("chinese academy sciences", "CAS", myCorpus[[j]])
# }
## let's convert to a plain text document before going further. We
## need this step to create the document term matrix
myCorpus = tm_map(myCorpus, PlainTextDocument)
#stem document
myCorpus = tm_map(myCorpus, stemDocument)
## create the term matrix
dtm = DocumentTermMatrix(myCorpus)
dtm
inspect(dtm[1:5, 1000:1005])
dim(dtm)
inspect(dtm[1:5, 100:105])
freq = colSums(as.matrix(dtm))
length(freq)
head(freq)
ord = order(freq)
head(ord)
freq[head(ord)]
freq[tail(ord)]
plot(table(freq))
table(freq)
m = as.matrix(dtm)
View(m)
ls()
rm(list = ls())
load("twitteR_credentials")
registerTwitterOAuth(twitCred)
## searching someone's timeline
## rdmTweets = userTimeline("rdatamining", n = 400, cainfo="cacert.pem")
## searching for a term
rdmTweets = searchTwitter("ebola", n = 400, cainfo="cacert.pem")
nDocs = length(rdmTweets)
## convert the tweets to a data frame
df = do.call("rbind", lapply(rdmTweets, as.data.frame))
## build a corpus
myCorpus = Corpus(VectorSource(df$text))
## cleanup
## make everything lowercase
myCorpus = tm_map(myCorpus, tolower)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
## remove numbers
## TODO: DECIDE IF YOU NEED TO REMOVE NUMBERS, OR IF YOU NEED THEM
## SKIP THIS STEP!!!
myCorpus <- tm_map(myCorpus, removeNumbers)
## inspecting the corpus
inspect(myCorpus[4:10])
## remove URLs
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
## replace '/', used sometimes to separate alternatives w a space
## replace '@' in email addresses
## replace '\\' from URLS
for (j in seq(myCorpus))
{
myCorpus[[j]] <- gsub("/", " ", myCorpus[[j]])
myCorpus[[j]] <- gsub("@", " ", myCorpus[[j]])
myCorpus[[j]] <- gsub("\\|", " ", myCorpus[[j]])
}
## add two extra stop words: "available" and "via"
myStopwords <- c(stopwords('english'), "available", "via")
## remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, c("r", "big"))
## remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
## strip whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
## task-specific transformations (pw to pathway, etc)
# for (j in seq(myCorpus))
# {
#     myCorpus[[j]] <- gsub("harbin institute technology", "HIT", myCorpus[[j]])
#     myCorpus[[j]] <- gsub("shenzhen institutes advanced technology", "SIAT", myCorpus[[j]])
#     myCorpus[[j]] <- gsub("chinese academy sciences", "CAS", myCorpus[[j]])
# }
## let's convert to a plain text document before going further. We
## need this step to create the document term matrix
myCorpus = tm_map(myCorpus, PlainTextDocument)
#stem document
myCorpus = tm_map(myCorpus, stemDocument)
## create the term matrix
dtm = DocumentTermMatrix(myCorpus)
## get term frequencies
freq = colSums(as.matrix(dtm))
m = as.matrix(dtm)
View(m)
View(df)
head(df)
head(myCorpus)
inspect(myCorpus[[100:105]])
inspect(myCorpus[100:105])
rdmTweets = searchTwitter("ebola", n = 400, cainfo="cacert.pem", language="english")
rdmTweets = searchTwitter("ebola", n = 400, cainfo="cacert.pem", language="en")
rdmTweets = searchTwitter("ebola", n = 400, cainfo="cacert.pem", lang="en")
nDocs = length(rdmTweets)
## convert the tweets to a data frame
df = do.call("rbind", lapply(rdmTweets, as.data.frame))
## build a corpus
myCorpus = Corpus(VectorSource(df$text))
## cleanup
## make everything lowercase
myCorpus = tm_map(myCorpus, tolower)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
## remove numbers
## TODO: DECIDE IF YOU NEED TO REMOVE NUMBERS, OR IF YOU NEED THEM
## SKIP THIS STEP!!!
myCorpus <- tm_map(myCorpus, removeNumbers)
## inspecting the corpus
inspect(myCorpus[4:10])
## remove URLs
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
## replace '/', used sometimes to separate alternatives w a space
## replace '@' in email addresses
## replace '\\' from URLS
for (j in seq(myCorpus))
{
myCorpus[[j]] <- gsub("/", " ", myCorpus[[j]])
myCorpus[[j]] <- gsub("@", " ", myCorpus[[j]])
myCorpus[[j]] <- gsub("\\|", " ", myCorpus[[j]])
}
## add two extra stop words: "available" and "via"
myStopwords <- c(stopwords('english'), "available", "via")
## remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, c("r", "big"))
## remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
## strip whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
## task-specific transformations (pw to pathway, etc)
# for (j in seq(myCorpus))
# {
#     myCorpus[[j]] <- gsub("harbin institute technology", "HIT", myCorpus[[j]])
#     myCorpus[[j]] <- gsub("shenzhen institutes advanced technology", "SIAT", myCorpus[[j]])
#     myCorpus[[j]] <- gsub("chinese academy sciences", "CAS", myCorpus[[j]])
# }
## let's convert to a plain text document before going further. We
## need this step to create the document term matrix
myCorpus = tm_map(myCorpus, PlainTextDocument)
#stem document
myCorpus = tm_map(myCorpus, stemDocument)
## create the term matrix
dtm = DocumentTermMatrix(myCorpus)
## get term frequencies
freq = colSums(as.matrix(dtm))
m = as.matrix(dtm)
View(df)
View(m)
dim(m)
write.csv(m, "C://data//wordmatrix.csv")
dtm2 = removeSparseTerms(dtm, 0.1)
dim(dtm)
dim(dtm2)
names(dtm2)
dtm3 = removeSparseTerms(dtm, 0.9)
dim(dtm3)
names(dtm3)
dtm3$Terms
dtm3 = removeSparseTerms(dtm, 0.99)
dim(dtm3)
freq = colSums(as.matrix(dtm3))
freq
freq = colSums(as.matrix(dtm2))
freq
dtm3 = removeSparseTerms(dtm, 0.9)
freq = colSums(as.matrix(dtm3))
freq
findFreqTerms(dtm, lowfreq = 200)
findFreqTerms(dtm, lowfreq = 20)
findAssocs(dtm, "treat", corlimit = 0.3)
plot(dtm, terms=findFreqTerms(dtm, lowfreq=40)[1:50], corThreshold=0.5)
source("http://bioconductor.org/biocLite.R")
biocLite("Rgraphviz")
plot(dtm, terms=findFreqTerms(dtm, lowfreq=40)[1:50], corThreshold=0.5)
plot(dtm, terms=findFreqTerms(dtm, lowfreq=40)[1:50], corThreshold=0.5)
plot(dtm, terms=findFreqTerms(dtm, lowfreq=20)[1:50], corThreshold=0.5)
plot(dtm, terms=findFreqTerms(dtm, lowfreq=20)[1:10], corThreshold=0.5)
library(wordcloud)
install.packages("wordcloud")
library(wordcloud)
wordcloud(names(freq), freq, min.freq(10))
wordcloud(names(freq), freq, min.freq=10)
wordcloud(names(freq), freq, min.freq=5)
wordcloud(names(freq), freq, min.freq=3)
wordcloud(names(freq), freq, min.freq=2)
wordcloud(names(freq), freq, min.freq=1)
table(freq)
freq
freq = colSums(as.matrix(dtm))
wordcloud(names(freq), freq, min.freq=1)
wordcloud(names(freq), freq, min.freq=10)
wordcloud(names(freq), freq, min.freq=50)
wordcloud(names(freq), freq, min.freq=20)
wordcloud(names(freq), freq, min.freq=10)
wordcloud(names(freq), freq, min.freq=10, colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=10, scale=c(5, 0.1), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=10, scale=c(5, 0.2), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=10, scale=c(5, 0.2), colors=brewer.pal(6, "Dark2"))
rm(list = ls())
load("twitteR_credentials")
registerTwitterOAuth(twitCred)
rdmTweets = searchTwitter("saints", n = 400, cainfo="cacert.pem", lang="en")
nDocs = length(rdmTweets)
## convert the tweets to a data frame
df = do.call("rbind", lapply(rdmTweets, as.data.frame))
## build a corpus
myCorpus = Corpus(VectorSource(df$text))
## cleanup
## make everything lowercase
myCorpus = tm_map(myCorpus, tolower)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
inspect(myCorpus[4:10])
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
## replace '/', used sometimes to separate alternatives w a space
myStopwords <- c(stopwords('english'), "available", "via")
## remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, c("r", "big"))
## remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
## strip whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
inspect(myCorpus[4:10])
myCorpusCopy = myCorpus
## let's convert to a plain text document before going further. We
## need this step to create the document term matrix
myCorpus = tm_map(myCorpus, PlainTextDocument)
#stem document
myCorpus = tm_map(myCorpus, stemDocument)
inspect(myCorpus[4:10])
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=myCorpusCopy)
myCorpus = tm_map(myCorpus, PlainTextDocument)
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=myCorpusCopy)
myCorpusCopy = tm_map(myCorpusCopy, PlainTextDocument)
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=myCorpusCopy)
inspect(myCorpus[41:50])
dtm = DocumentTermMatrix(myCorpus)
dtm = DocumentTermMatrix(myCorpus)
myCorpus = tm_map(myCorpus, PlainTextDocument)
dtm = DocumentTermMatrix(myCorpus)
findFreqTerms(dtm, lowfreq=100)
findFreqTerms(dtm, lowfreq=50)
findFreqTerms(dtm, lowfreq=10)
findAssocs(dtm, "data", corlimit=0.6)
findAssocs(dtm, "brees", corlimit=0.6)
inspect(myCorpus[41:50])
myCorpus = Corpus(VectorSource(df$text))
## cleanup
## make everything lowercase
myCorpus = tm_map(myCorpus, tolower)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
## remove numbers
inspect(myCorpus[41:50])
## remove URLs
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
myStopwords <- c(stopwords('english'), "available", "via")
## remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, c("r", "big"))
## remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
## strip whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus = tm_map(myCorpus, PlainTextDocument)
#stem document
myCorpus = tm_map(myCorpus, stemDocument)
inspect(myCorpus[41:50])
dtm = DocumentTermMatrix(myCorpus)
# Explore the corpus.
findFreqTerms(dtm, lowfreq=10)
findAssocs(dtm, "brees", corlimit=0.6)
findAssocs(dtm, "atlanta", corlimit=0.6)
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
wf <- data.frame(word=names(freq), freq=freq)
library(ggplot2)
p <- ggplot(subset(wf, freq>50), aes(word, freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
# Generate a word cloud
library(wordcloud)
wordcloud(names(freq), freq, min.freq=100, colors=brewer.pal(6, "Dark2")
wordcloud(names(freq), freq, min.freq=100, colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=100, colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=10, colors=brewer.pal(6, "Dark2"))
p <- ggplot(subset(wf, freq>50), aes(word, freq))
load("twitteR_credentials")
registerTwitterOAuth(twitCred)
## searching someone's timeline
## rdmTweets = userTimeline("rdatamining", n = 400, cainfo="cacert.pem")
## searching for a term
rdmTweets = searchTwitter("Obama", n = 400, cainfo="cacert.pem", lang="en")
nDocs = length(rdmTweets)
## convert the tweets to a data frame
df = do.call("rbind", lapply(rdmTweets, as.data.frame))
## build a corpus
myCorpus = Corpus(VectorSource(df$text))
## cleanup
## make everything lowercase
myCorpus = tm_map(myCorpus, tolower)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
## remove numbers
inspect(myCorpus[41:50])
myCorpus <- tm_map(myCorpus, removeNumbers)
## inspecting the corpus
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
## replace '/', used sometimes to separate alternatives w a space
## replace '@' in email addresses
## replace '\\' from URLS
myStopwords <- c(stopwords('english'), "available", "via", "obama")
## remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, c("r", "big"))
## remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
## strip whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus = tm_map(myCorpus, PlainTextDocument)
#stem document
myCorpus = tm_map(myCorpus, stemDocument)
inspect(myCorpus[41:50])
myCorpusCopy = myCorpus
myCorpusCopy = tm_map(myCorpusCopy, PlainTextDocument)
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=myCorpusCopy)
warnings()
inspect(myCorpus[41:50])
myCorpus <- tm_map(myCorpus, stripWhitespace)
inspect(myCorpus[41:50])
dtm = DocumentTermMatrix(myCorpus)
myCorpus = tm_map(myCorpus, PlainTextDocument)
dtm = DocumentTermMatrix(myCorpus)
findFreqTerms(dtm, lowfreq=10)
findAssocs(dtm, "obama", corlimit=0.6)
findAssocs(dtm, "potus", corlimit=0.6)
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
wf <- data.frame(word=names(freq), freq=freq)
inspect(myCorpus[41:50])
myCorpus = Corpus(VectorSource(df$text))
## cleanup
## make everything lowercase
myCorpus = tm_map(myCorpus, tolower)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
## remove numbers
## TODO: DECIDE IF YOU NEED TO REMOVE NUMBERS, OR IF YOU NEED THEM
## SKIP THIS STEP!!!
myCorpus <- tm_map(myCorpus, removeNumbers)
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
myStopwords <- c(stopwords('english'), "available", "via", "obama")
## remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, c("r", "big"))
## remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
## strip whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus = tm_map(myCorpus, PlainTextDocument)
#stem document
myCorpus = tm_map(myCorpus, stemDocument)
dtm = DocumentTermMatrix(myCorpus)
# Explore the corpus.
findFreqTerms(dtm, lowfreq=10)
myCorpusCopy = myCorpus
myCorpusCopy = tm_map(myCorpusCopy, PlainTextDocument)
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=myCorpusCopy, type = prevalent)
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=myCorpusCopy, type = "prevalent")
warnings()
myCorpus = Corpus(VectorSource(df$text))
## cleanup
## make everything lowercase
myCorpus = tm_map(myCorpus, tolower)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
## remove numbers
## TODO: DECIDE IF YOU NEED TO REMOVE NUMBERS, OR IF YOU NEED THEM
## SKIP THIS STEP!!!
myCorpus <- tm_map(myCorpus, removeNumbers)
## inspecting the corpus
inspect(myCorpus[41:50])
## remove URLs
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
myStopwords <- c(stopwords('english'), "available", "via", "obama")
## remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, c("r", "big"))
## remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
## strip whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus = tm_map(myCorpus, PlainTextDocument)
#stem document
myCorpus = tm_map(myCorpus, stemDocument)
dtm = DocumentTermMatrix(myCorpus)
# Explore the corpus.
findFreqTerms(dtm, lowfreq=10)
findAssocs(dtm, "potus", corlimit=0.6)
findAssocs(dtm, "lesbian", corlimit=0.6)
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
wf <- data.frame(word=names(freq), freq=freq)
library(ggplot2)
p <- ggplot(subset(wf, freq>50), aes(word, freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
# Generate a word cloud
library(wordcloud)
wordcloud(names(freq), freq, min.freq=10, colors=brewer.pal(6, "Dark2"))
myTdm <- TermDocumentMatrix(myCorpus, control=list(wordLengths=c(1,Inf)))
myTdm
idx <- which(dimnames(myTdm)$Terms == "r")
inspect(myTdm[idx+(0:5),101:110])
findFreqTerms(myTdm, lowfreq=10)
termFrequency <- rowSums(as.matrix(myTdm))
termFrequency <- subset(termFrequency, termFrequency>=10)
qplot(names(termFrequency), termFrequency, geom="bar", xlab="Terms") + coord_flip()
qplot(names(termFrequency), geom="bar", xlab="Terms") + coord_flip()
termFrequency
qplot(names(termFrequency), geom="bar", xlab="Terms", ylim = 35) + coord_flip()
qplot(names(termFrequency), geom="bar", xlab="Terms", xlim = 35) + coord_flip()
qplot(names(termFrequency), geom="bar", xlab="Terms", ylim = 35, xlim=length(termFrequency)) + coord_flip()
qplot(names(termFrequency), geom="bar", xlab="Terms") #+ coord_flip()
qplot(names(termFrequency), geom="bar", xlab="Terms") + coord_flip()
library(ggplot2)
qplot(names(termFrequency), geom="bar", xlab="Terms") + coord_flip()
myTdm2 = removeSparseTerms(myTdm, sparse=0.95)
dim(myTdm2)
m2 = as.matrix(myTdm2)
distMatrix = dist(scale(m2))
fit = hclust(distMatrix, method="ward")
fit = hclust(distMatrix, method="ward.D2")
plot(fit)
rect.hclust(fit, k=10)
acs = read.csv("c:\\data\\acs.csv")
rm(list = ls())
acs = read.csv("c:\\data\\acs.csv")
names(acs)
sset = acs[which(acs$VAL == 24), ]
download.file(url = "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx", destfile = "c:\\data\\ngap.xlsx", mode = "wb")
rows = 18:23
cols = 7:15
dat = read.xlsx("c:\\data\\ngap.xlsx", sheetIndex = 1, colIndex = cols, rowIndex = rows)
library(xlsx)
dat = read.xlsx("c:\\data\\ngap.xlsx", sheetIndex = 1, colIndex = cols, rowIndex = rows)
sum(dat$Zip*dat$Ext,na.rm=T)
download.file(url = "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml", destfile = "c:\\data\\restaurants.xml")
DT = fread("c:\\data\\restaurants.xml", sep="auto", sep2="auto", nrows=-1L, header="auto", na.strings="NA",
stringsAsFactors=FALSE, verbose=FALSE, autostart=30L, skip=-1L, select=NULL,
colClasses=NULL, integer64=getOption("datatable.integer64"))
library(data.table)
rm(list = ls())
rm(df)
rm(list = ls())
mem()
ls()
data()
str(Nile)
ToothGrowth?
?ToothGrowth
str(ToothGrowth)
ToothGrowth
qplot(dose, len, data=ToothGrowth)
library(ggplot2)
qplot(dose, len, data=ToothGrowth)
qplot(dose, len, data=ToothGrowth, color = supp)
qplot(dose, len, data=ToothGrowth, color = supp, geom = c("point", "smooth"), method = "lm")
library(lattice)
library(ggplot2)
library(plyr)
## set working directory
setwd(".\\Github\\EDA-Project2")
## assumes that the source data files are in a subfolder named 'data'
## NEI will contain all of the PM2.5 emissions data for 1999, 2002, 2005, and 2008.
## For each year, the table contains number of tons of PM2.5 emitted from a specific
## type of source for the entire year.
install.packages(c("digest", "httr", "quantmod", "RColorBrewer", "RCurl", "reshape2"))
install.packages(c("digest", "httr", "quantmod", "RColorBrewer",
